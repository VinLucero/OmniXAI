<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>omnixai.explainers.vision package &mdash; OmniXAI  documentation</title>
      <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
      <link rel="stylesheet" href="_static/css/theme.css" type="text/css" />
      <link rel="stylesheet" href="_static/theme.css" type="text/css" />
  <!--[if lt IE 9]>
    <script src="_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script>
        <script src="_static/jquery.js"></script>
        <script src="_static/underscore.js"></script>
        <script src="_static/doctools.js"></script>
        <script crossorigin="anonymous" integrity="sha256-Ae2Vz/4ePdIu6ZyI/5ZGsYnb+m0JlOmKPjt6XZ9JJkA=" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
    <script src="_static/js/theme.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="omnixai.explainers.vision.agnostic package" href="omnixai.explainers.vision.agnostic.html" />
    <link rel="prev" title="omnixai.explainers.tabular.specific package" href="omnixai.explainers.tabular.specific.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
            <a href="index.html" class="icon icon-home"> OmniXAI
          </a>
              <div class="version">
                latest
              </div>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Contents:</span></p>
<ul class="current">
<li class="toctree-l1 current"><a class="reference internal" href="omnixai.html">OmniXAI: An Explanation Toolbox</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="omnixai.html#overview">Overview</a></li>
<li class="toctree-l2"><a class="reference internal" href="omnixai.html#installation">Installation</a></li>
<li class="toctree-l2"><a class="reference internal" href="omnixai.html#getting-started">Getting Started</a></li>
<li class="toctree-l2"><a class="reference internal" href="omnixai.html#how-to-contribute">How to Contribute</a></li>
<li class="toctree-l2"><a class="reference internal" href="omnixai.html#modules-for-different-data-types">Modules for Different Data Types</a></li>
<li class="toctree-l2"><a class="reference internal" href="omnixai.html#preprocessing-functions">Preprocessing Functions</a></li>
<li class="toctree-l2 current"><a class="reference internal" href="omnixai.html#supported-explanation-methods">Supported Explanation Methods</a><ul class="current">
<li class="toctree-l3 current"><a class="reference internal" href="omnixai.explainers.html">omnixai.explainers package</a><ul class="current">
<li class="toctree-l4"><a class="reference internal" href="omnixai.explainers.html#module-omnixai.explainers.base">omnixai.explainers.base module</a></li>
<li class="toctree-l4 current"><a class="reference internal" href="omnixai.explainers.html#explainers-for-different-tasks">Explainers for different tasks</a><ul class="current">
<li class="toctree-l5"><a class="reference internal" href="omnixai.explainers.tabular.html">omnixai.explainers.tabular package</a></li>
<li class="toctree-l5 current"><a class="current reference internal" href="#">omnixai.explainers.vision package</a><ul>
<li class="toctree-l6"><a class="reference internal" href="#module-omnixai.explainers.vision.auto">omnixai.explainers.vision.auto module</a></li>
<li class="toctree-l6"><a class="reference internal" href="#subpackages">Subpackages</a><ul>
<li class="toctree-l7"><a class="reference internal" href="omnixai.explainers.vision.agnostic.html">omnixai.explainers.vision.agnostic package</a></li>
<li class="toctree-l7"><a class="reference internal" href="omnixai.explainers.vision.specific.html">omnixai.explainers.vision.specific package</a></li>
<li class="toctree-l7"><a class="reference internal" href="omnixai.explainers.vision.counterfactual.html">omnixai.explainers.vision.counterfactual package</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l5"><a class="reference internal" href="omnixai.explainers.nlp.html">omnixai.explainers.nlp package</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="omnixai.html#modules-for-explanation-results">Modules for Explanation Results</a></li>
<li class="toctree-l2"><a class="reference internal" href="omnixai.html#dashboard-for-visualization">Dashboard for Visualization</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="tutorials.html">Tutorials &amp; Example Code</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="index.html">OmniXAI</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="index.html" class="icon icon-home"></a> &raquo;</li>
          <li><a href="omnixai.html">OmniXAI: An Explanation Toolbox</a> &raquo;</li>
          <li><a href="omnixai.explainers.html">omnixai.explainers package</a> &raquo;</li>
      <li>omnixai.explainers.vision package</li>
      <li class="wy-breadcrumbs-aside">
            <a href="_sources/omnixai.explainers.vision.rst.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  
<style>
/* CSS overrides for sphinx_rtd_theme */

/* 24px margin */
.nbinput.nblast.container,
.nboutput.nblast.container {
    margin-bottom: 19px;  /* padding has already 5px */
}

/* ... except between code cells! */
.nblast.container + .nbinput.container {
    margin-top: -19px;
}

.admonition > p:before {
    margin-right: 4px;  /* make room for the exclamation icon */
}

/* Fix math alignment, see https://github.com/rtfd/sphinx_rtd_theme/pull/686 */
.math {
    text-align: unset;
}
</style>
<section id="module-omnixai.explainers.vision">
<span id="omnixai-explainers-vision-package"></span><h1>omnixai.explainers.vision package<a class="headerlink" href="#module-omnixai.explainers.vision" title="Permalink to this headline"></a></h1>
<dl class="py class">
<dt class="sig sig-object py" id="omnixai.explainers.vision.VisionExplainer">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">omnixai.explainers.vision.</span></span><span class="sig-name descname"><span class="pre">VisionExplainer</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">explainers</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">mode</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">model</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">data</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">preprocess</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">postprocess</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">params</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#omnixai.explainers.vision.VisionExplainer" title="Permalink to this definition"></a></dt>
<dd><p>Bases: <a class="reference internal" href="omnixai.explainers.html#omnixai.explainers.base.AutoExplainerBase" title="omnixai.explainers.base.AutoExplainerBase"><code class="xref py py-class docutils literal notranslate"><span class="pre">omnixai.explainers.base.AutoExplainerBase</span></code></a></p>
<p>The class derived from <cite>AutoExplainerBase</cite> for image data,
allowing users to choose multiple explainers and generate
different explanations at the same time.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">explainer</span> <span class="o">=</span> <span class="n">VisionExplainer</span><span class="p">(</span>
    <span class="n">explainers</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;gradcam&quot;</span><span class="p">,</span> <span class="s2">&quot;lime&quot;</span><span class="p">,</span> <span class="s2">&quot;ig&quot;</span><span class="p">],</span>
    <span class="n">mode</span><span class="o">=</span><span class="s2">&quot;classification&quot;</span><span class="p">,</span>
    <span class="n">model</span><span class="o">=</span><span class="n">model</span><span class="p">,</span>
    <span class="n">preprocess</span><span class="o">=</span><span class="n">preprocess_function</span><span class="p">,</span>
    <span class="n">postprocess</span><span class="o">=</span><span class="n">postprocess_function</span><span class="p">,</span>
    <span class="n">params</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;gradcam&quot;</span><span class="p">:</span> <span class="p">{</span><span class="s2">&quot;target_layer&quot;</span><span class="p">:</span> <span class="n">model</span><span class="o">.</span><span class="n">layer4</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]}}</span>
<span class="p">)</span>
<span class="n">local_explanations</span> <span class="o">=</span> <span class="n">explainer</span><span class="o">.</span><span class="n">explain</span><span class="p">(</span><span class="n">img</span><span class="p">)</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>explainers</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">Collection</span></code>) – The names or alias of the explainers to use.</p></li>
<li><p><strong>mode</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>) – The task type, e.g. classification or regression.</p></li>
<li><p><strong>model</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Any</span></code>) – The machine learning model which can be a scikit-learn model,
a tensorflow model, a torch model, or a prediction function.</p></li>
<li><p><strong>data</strong> (<a class="reference internal" href="omnixai.data.html#omnixai.data.image.Image" title="omnixai.data.image.Image"><code class="xref py py-class docutils literal notranslate"><span class="pre">Image</span></code></a>) – The training data used to initialize explainers.
It can be empty, e.g., <cite>data = Image()</cite>, for those explainers such as
<cite>IntegratedGradient</cite> and <cite>Grad-CAM</cite> that don’t require training data.</p></li>
<li><p><strong>preprocess</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Optional</span></code>[<code class="xref py py-data docutils literal notranslate"><span class="pre">Callable</span></code>]) – The preprocessing function that converts the raw data
into the inputs of <code class="docutils literal notranslate"><span class="pre">model</span></code>.</p></li>
<li><p><strong>postprocess</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Optional</span></code>[<code class="xref py py-data docutils literal notranslate"><span class="pre">Callable</span></code>]) – The postprocessing function that transforms the outputs of <code class="docutils literal notranslate"><span class="pre">model</span></code>
to a user-specific form, e.g., the predicted probability for each class.</p></li>
<li><p><strong>params</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Optional</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">Dict</span></code>]) – A dict containing the additional parameters for initializing each explainer,
e.g., <cite>params[“lime”] = {“param_1”: param_1, …}</cite>.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="omnixai.explainers.vision.LimeImage">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">omnixai.explainers.vision.</span></span><span class="sig-name descname"><span class="pre">LimeImage</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">predict_function</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">mode</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'classification'</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#omnixai.explainers.vision.LimeImage" title="Permalink to this definition"></a></dt>
<dd><p>Bases: <a class="reference internal" href="omnixai.explainers.html#omnixai.explainers.base.ExplainerBase" title="omnixai.explainers.base.ExplainerBase"><code class="xref py py-class docutils literal notranslate"><span class="pre">omnixai.explainers.base.ExplainerBase</span></code></a></p>
<p>The LIME explainer for image data.
If using this explainer, please cite the original work: <a class="reference external" href="https://github.com/marcotcr/lime">https://github.com/marcotcr/lime</a>.
This explainer only supports image classification.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>predict_function</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Callable</span></code>) – The prediction function corresponding to the machine learning
model to explain. For classification, the outputs of the <code class="docutils literal notranslate"><span class="pre">predict_function</span></code>
are the class probabilities.</p></li>
<li><p><strong>mode</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>) – The task type which can be <cite>classification</cite> only.</p></li>
<li><p><strong>kwargs</strong> – Not used here.</p></li>
</ul>
</dd>
</dl>
<dl class="py attribute">
<dt class="sig sig-object py" id="omnixai.explainers.vision.LimeImage.explanation_type">
<span class="sig-name descname"><span class="pre">explanation_type</span></span><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">'local'</span></em><a class="headerlink" href="#omnixai.explainers.vision.LimeImage.explanation_type" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="omnixai.explainers.vision.LimeImage.alias">
<span class="sig-name descname"><span class="pre">alias</span></span><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">['lime']</span></em><a class="headerlink" href="#omnixai.explainers.vision.LimeImage.alias" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="omnixai.explainers.vision.LimeImage.explain">
<span class="sig-name descname"><span class="pre">explain</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#omnixai.explainers.vision.LimeImage.explain" title="Permalink to this definition"></a></dt>
<dd><p>Generates the explanations for the input instances.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>X</strong> – A batch of input instances.</p></li>
<li><p><strong>kwargs</strong> – Additional parameters, e.g., <code class="docutils literal notranslate"><span class="pre">top_labels</span></code> – the number
of the top labels to explain. Please refer to the doc of
<cite>LimeImageExplainer.explain_instance</cite>.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>The explanations for all the input instances.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><a class="reference internal" href="omnixai.explanations.image.html#omnixai.explanations.image.mask.MaskExplanation" title="omnixai.explanations.image.mask.MaskExplanation">MaskExplanation</a></p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="omnixai.explainers.vision.ShapImage">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">omnixai.explainers.vision.</span></span><span class="sig-name descname"><span class="pre">ShapImage</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">model</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">preprocess_function</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">mode</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'classification'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">background_data</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#omnixai.explainers.vision.ShapImage" title="Permalink to this definition"></a></dt>
<dd><p>Bases: <a class="reference internal" href="omnixai.explainers.html#omnixai.explainers.base.ExplainerBase" title="omnixai.explainers.base.ExplainerBase"><code class="xref py py-class docutils literal notranslate"><span class="pre">omnixai.explainers.base.ExplainerBase</span></code></a></p>
<p>The SHAP explainer for image data.
If using this explainer, please cite the original work: <a class="reference external" href="https://github.com/slundberg/shap">https://github.com/slundberg/shap</a>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>model</strong> – The model whose type can be <cite>tf.keras.Model</cite> or <cite>torch.nn.Module</cite>.</p></li>
<li><p><strong>preprocess_function</strong> – The preprocessing function that converts the raw data
into the inputs of <code class="docutils literal notranslate"><span class="pre">model</span></code>.</p></li>
<li><p><strong>mode</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>) – The task type, e.g., <cite>classification</cite> or <cite>regression</cite>.</p></li>
<li><p><strong>background_data</strong> (<a class="reference internal" href="omnixai.data.html#omnixai.data.image.Image" title="omnixai.data.image.Image"><code class="xref py py-class docutils literal notranslate"><span class="pre">Image</span></code></a>) – The background images to compare with.</p></li>
<li><p><strong>kwargs</strong> – Not used here.</p></li>
</ul>
</dd>
</dl>
<dl class="py attribute">
<dt class="sig sig-object py" id="omnixai.explainers.vision.ShapImage.explanation_type">
<span class="sig-name descname"><span class="pre">explanation_type</span></span><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">'local'</span></em><a class="headerlink" href="#omnixai.explainers.vision.ShapImage.explanation_type" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="omnixai.explainers.vision.ShapImage.alias">
<span class="sig-name descname"><span class="pre">alias</span></span><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">['shap']</span></em><a class="headerlink" href="#omnixai.explainers.vision.ShapImage.alias" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="omnixai.explainers.vision.ShapImage.explain">
<span class="sig-name descname"><span class="pre">explain</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">y</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#omnixai.explainers.vision.ShapImage.explain" title="Permalink to this definition"></a></dt>
<dd><p>Generates the pixel-importance explanations for the input instances.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>X</strong> (<a class="reference internal" href="omnixai.data.html#omnixai.data.image.Image" title="omnixai.data.image.Image"><code class="xref py py-class docutils literal notranslate"><span class="pre">Image</span></code></a>) – A batch of input instances.</p></li>
<li><p><strong>y</strong> – A batch of labels to explain. For regression, <code class="docutils literal notranslate"><span class="pre">y</span></code> is ignored.
For classification, the top predicted label of each input instance will be explained
when <cite>y = None</cite>.</p></li>
<li><p><strong>kwargs</strong> – Additional parameters, e.g., <code class="docutils literal notranslate"><span class="pre">nsamples</span></code> – the maximum number of images
sampled for the background.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>The explanations for all the input instances, e.g., pixel importance scores.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><a class="reference internal" href="omnixai.explanations.image.html#omnixai.explanations.image.pixel_importance.PixelImportance" title="omnixai.explanations.image.pixel_importance.PixelImportance">PixelImportance</a></p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="omnixai.explainers.vision.IntegratedGradientImage">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">omnixai.explainers.vision.</span></span><span class="sig-name descname"><span class="pre">IntegratedGradientImage</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">model</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">preprocess_function</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">mode</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'classification'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">background_data</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#omnixai.explainers.vision.IntegratedGradientImage" title="Permalink to this definition"></a></dt>
<dd><p>Bases: <a class="reference internal" href="omnixai.explainers.html#omnixai.explainers.base.ExplainerBase" title="omnixai.explainers.base.ExplainerBase"><code class="xref py py-class docutils literal notranslate"><span class="pre">omnixai.explainers.base.ExplainerBase</span></code></a>, <a class="reference internal" href="omnixai.explainers.tabular.specific.html#omnixai.explainers.tabular.specific.ig.IntegratedGradient" title="omnixai.explainers.tabular.specific.ig.IntegratedGradient"><code class="xref py py-class docutils literal notranslate"><span class="pre">omnixai.explainers.tabular.specific.ig.IntegratedGradient</span></code></a></p>
<p>The integrated-gradient explainer for image data.
If using this explainer, please cite the original work: <a class="reference external" href="https://github.com/ankurtaly/Integrated-Gradients">https://github.com/ankurtaly/Integrated-Gradients</a>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>model</strong> – The model whose type can be <cite>tf.keras.Model</cite> or <cite>torch.nn.Module</cite>.</p></li>
<li><p><strong>preprocess_function</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Callable</span></code>) – The pre-processing function that converts the raw input data
into the inputs of <code class="docutils literal notranslate"><span class="pre">model</span></code>.</p></li>
<li><p><strong>mode</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>) – The task type, e.g., <cite>classification</cite> or <cite>regression</cite>.</p></li>
<li><p><strong>background_data</strong> (<a class="reference internal" href="omnixai.data.html#omnixai.data.image.Image" title="omnixai.data.image.Image"><code class="xref py py-class docutils literal notranslate"><span class="pre">Image</span></code></a>) – The background images to compare with. When <code class="docutils literal notranslate"><span class="pre">background_data</span></code>
is empty, the baselines for computing integrated gradients will be sampled randomly.</p></li>
<li><p><strong>kwargs</strong> – Additional parameters to initialize the IG explainer,
e.g., <code class="docutils literal notranslate"><span class="pre">num_random_trials</span></code> – the number of trials in generating baselines.</p></li>
</ul>
</dd>
</dl>
<dl class="py attribute">
<dt class="sig sig-object py" id="omnixai.explainers.vision.IntegratedGradientImage.explanation_type">
<span class="sig-name descname"><span class="pre">explanation_type</span></span><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">'local'</span></em><a class="headerlink" href="#omnixai.explainers.vision.IntegratedGradientImage.explanation_type" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="omnixai.explainers.vision.IntegratedGradientImage.alias">
<span class="sig-name descname"><span class="pre">alias</span></span><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">['ig',</span> <span class="pre">'integrated_gradient']</span></em><a class="headerlink" href="#omnixai.explainers.vision.IntegratedGradientImage.alias" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="omnixai.explainers.vision.IntegratedGradientImage.explain">
<span class="sig-name descname"><span class="pre">explain</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">y</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">baseline</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#omnixai.explainers.vision.IntegratedGradientImage.explain" title="Permalink to this definition"></a></dt>
<dd><p>Generates the pixel-importance explanations for the input instances.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>X</strong> (<a class="reference internal" href="omnixai.data.html#omnixai.data.image.Image" title="omnixai.data.image.Image"><code class="xref py py-class docutils literal notranslate"><span class="pre">Image</span></code></a>) – A batch of input instances.</p></li>
<li><p><strong>y</strong> – A batch of labels to explain. For regression, <code class="docutils literal notranslate"><span class="pre">y</span></code> is ignored.
For classification, the top predicted label of each input instance will be explained
when <code class="docutils literal notranslate"><span class="pre">y</span> <span class="pre">=</span> <span class="pre">None</span></code>.</p></li>
<li><p><strong>baseline</strong> – The baselines for computing integrated gradients. When it is <cite>None</cite>,
the baselines will be sampled randomly.</p></li>
<li><p><strong>kwargs</strong> – Additional parameters, e.g., <code class="docutils literal notranslate"><span class="pre">steps</span></code> for
<cite>IntegratedGradient.compute_integrated_gradients</cite>.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>The explanations for all the instances, e.g., pixel importance scores.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><a class="reference internal" href="omnixai.explanations.image.html#omnixai.explanations.image.pixel_importance.PixelImportance" title="omnixai.explanations.image.pixel_importance.PixelImportance">PixelImportance</a></p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="omnixai.explainers.vision.PartialDependenceImage">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">omnixai.explainers.vision.</span></span><span class="sig-name descname"><span class="pre">PartialDependenceImage</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">predict_function</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">mode</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'classification'</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#omnixai.explainers.vision.PartialDependenceImage" title="Permalink to this definition"></a></dt>
<dd><p>Bases: <a class="reference internal" href="omnixai.explainers.html#omnixai.explainers.base.ExplainerBase" title="omnixai.explainers.base.ExplainerBase"><code class="xref py py-class docutils literal notranslate"><span class="pre">omnixai.explainers.base.ExplainerBase</span></code></a></p>
<p>The partial dependence plots for image data. The input image is segmented by a particular
segmentation method, e.g., “quickshift”. For each segment, its importance score is measured
by the average change of the predicted value when the segment is replaced by new segments constructed
in the grid search.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>predict_function</strong> – The prediction function corresponding to the model to explain.
When the model is for classification, the outputs of the <code class="docutils literal notranslate"><span class="pre">predict_function</span></code>
are the class probabilities. When the model is for regression, the outputs of
the <code class="docutils literal notranslate"><span class="pre">predict_function</span></code> are the estimated values.</p></li>
<li><p><strong>mode</strong> – The task type, e.g., <cite>classification</cite> or <cite>regression</cite>.</p></li>
<li><p><strong>kwargs</strong> – Not used here.</p></li>
</ul>
</dd>
</dl>
<dl class="py attribute">
<dt class="sig sig-object py" id="omnixai.explainers.vision.PartialDependenceImage.explanation_type">
<span class="sig-name descname"><span class="pre">explanation_type</span></span><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">'local'</span></em><a class="headerlink" href="#omnixai.explainers.vision.PartialDependenceImage.explanation_type" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="omnixai.explainers.vision.PartialDependenceImage.alias">
<span class="sig-name descname"><span class="pre">alias</span></span><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">['pdp',</span> <span class="pre">'partial_dependence']</span></em><a class="headerlink" href="#omnixai.explainers.vision.PartialDependenceImage.alias" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="omnixai.explainers.vision.PartialDependenceImage.explain">
<span class="sig-name descname"><span class="pre">explain</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">y</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#omnixai.explainers.vision.PartialDependenceImage.explain" title="Permalink to this definition"></a></dt>
<dd><p>Generates PDP explanations.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>X</strong> (<a class="reference internal" href="omnixai.data.html#omnixai.data.image.Image" title="omnixai.data.image.Image"><code class="xref py py-class docutils literal notranslate"><span class="pre">Image</span></code></a>) – A batch of input instances.</p></li>
<li><p><strong>y</strong> – A batch of labels to explain. For regression, <code class="docutils literal notranslate"><span class="pre">y</span></code> is ignored.
For classification, the top predicted label of each input instance will be explained
when <code class="docutils literal notranslate"><span class="pre">y</span> <span class="pre">=</span> <span class="pre">None</span></code>.</p></li>
<li><p><strong>kwargs</strong> – Additional parameters in the PDP explainer, e.g., <code class="docutils literal notranslate"><span class="pre">grid_resolution</span></code> –
the resolution in the grid search, and <code class="docutils literal notranslate"><span class="pre">n_segments</span></code> – the number of image segments used
by image segmentation methods.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>The generated explanations, e.g., the importance scores for image segments.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><a class="reference internal" href="omnixai.explanations.image.html#omnixai.explanations.image.pixel_importance.PixelImportance" title="omnixai.explanations.image.pixel_importance.PixelImportance">PixelImportance</a></p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="omnixai.explainers.vision.L2XImage">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">omnixai.explainers.vision.</span></span><span class="sig-name descname"><span class="pre">L2XImage</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">training_data</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">predict_function</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">mode</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'classification'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">tau</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.5</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">k</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">10</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">selection_model</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">prediction_model</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">loss_function</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">optimizer</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">learning_rate</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.001</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch_size</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">num_epochs</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">20</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#omnixai.explainers.vision.L2XImage" title="Permalink to this definition"></a></dt>
<dd><p>Bases: <a class="reference internal" href="omnixai.explainers.html#omnixai.explainers.base.ExplainerBase" title="omnixai.explainers.base.ExplainerBase"><code class="xref py py-class docutils literal notranslate"><span class="pre">omnixai.explainers.base.ExplainerBase</span></code></a></p>
<p>The LIME explainer for image data.
If using this explainer, please cite the original work:
<cite>Learning to Explain: An Information-Theoretic Perspective on Model Interpretation,
Jianbo Chen, Le Song, Martin J. Wainwright, Michael I. Jordan, https://arxiv.org/abs/1802.07814</cite>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>training_data</strong> (<a class="reference internal" href="omnixai.data.html#omnixai.data.image.Image" title="omnixai.data.image.Image"><code class="xref py py-class docutils literal notranslate"><span class="pre">Image</span></code></a>) – The data used to train the explainer. <code class="docutils literal notranslate"><span class="pre">training_data</span></code>
should be the training dataset for training the machine learning model.</p></li>
<li><p><strong>predict_function</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Callable</span></code>) – The prediction function corresponding to the model to explain.
When the model is for classification, the outputs of the <code class="docutils literal notranslate"><span class="pre">predict_function</span></code>
are the class probabilities. When the model is for regression, the outputs of
the <code class="docutils literal notranslate"><span class="pre">predict_function</span></code> are the estimated values.</p></li>
<li><p><strong>mode</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>) – The task type, e.g., <cite>classification</cite> or <cite>regression</cite>.</p></li>
<li><p><strong>tau</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">float</span></code>) – Parameter <code class="docutils literal notranslate"><span class="pre">tau</span></code> in Gumbel-Softmax.</p></li>
<li><p><strong>k</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>) – The maximum number of the selected features in L2X.</p></li>
<li><p><strong>selection_model</strong> – A pytorch model class for estimating P(S|X) in L2X. If
<code class="docutils literal notranslate"><span class="pre">selection_model</span> <span class="pre">=</span> <span class="pre">None</span></code>, a default model <cite>DefaultSelectionModel</cite> will be used.</p></li>
<li><p><strong>prediction_model</strong> – A pytorch model class for estimating Q(X_S) in L2X. If
<code class="docutils literal notranslate"><span class="pre">prediction_model</span> <span class="pre">=</span> <span class="pre">None</span></code>, a default model <cite>DefaultPredictionModel</cite> will be used.</p></li>
<li><p><strong>loss_function</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Optional</span></code>[<code class="xref py py-data docutils literal notranslate"><span class="pre">Callable</span></code>]) – The loss function for the task, e.g., <cite>nn.CrossEntropyLoss()</cite>
for classification.</p></li>
<li><p><strong>optimizer</strong> – The optimizer class for training the explainer, e.g., <cite>torch.optim.Adam</cite>.</p></li>
<li><p><strong>learning_rate</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">float</span></code>) – The learning rate for training the explainer.</p></li>
<li><p><strong>batch_size</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Optional</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>]) – The batch size for training the explainer. If <code class="docutils literal notranslate"><span class="pre">batch_size</span></code> is <cite>None</cite>,
<code class="docutils literal notranslate"><span class="pre">batch_size</span></code> will be picked from <cite>[32, 64, 128, 256]</cite> based on the sample size.</p></li>
<li><p><strong>num_epochs</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>) – The number of epochs for training the explainer.</p></li>
<li><p><strong>kwargs</strong> – Additional parameters, e.g., parameters for <code class="docutils literal notranslate"><span class="pre">selection_model</span></code>
and <code class="docutils literal notranslate"><span class="pre">prediction_model</span></code>.</p></li>
</ul>
</dd>
</dl>
<dl class="py attribute">
<dt class="sig sig-object py" id="omnixai.explainers.vision.L2XImage.explanation_type">
<span class="sig-name descname"><span class="pre">explanation_type</span></span><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">'local'</span></em><a class="headerlink" href="#omnixai.explainers.vision.L2XImage.explanation_type" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="omnixai.explainers.vision.L2XImage.alias">
<span class="sig-name descname"><span class="pre">alias</span></span><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">['l2x',</span> <span class="pre">'L2X']</span></em><a class="headerlink" href="#omnixai.explainers.vision.L2XImage.alias" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="omnixai.explainers.vision.L2XImage.explain">
<span class="sig-name descname"><span class="pre">explain</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#omnixai.explainers.vision.L2XImage.explain" title="Permalink to this definition"></a></dt>
<dd><p>Generates the explanations for the input instances. For classification,
it explains the top predicted label for each input instance.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>X</strong> (<a class="reference internal" href="omnixai.data.html#omnixai.data.image.Image" title="omnixai.data.image.Image"><code class="xref py py-class docutils literal notranslate"><span class="pre">Image</span></code></a>) – A batch of input instances.</p></li>
<li><p><strong>kwargs</strong> – Additional parameters, not used here.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>The explanations for all the input instances.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><a class="reference internal" href="omnixai.explanations.image.html#omnixai.explanations.image.pixel_importance.PixelImportance" title="omnixai.explanations.image.pixel_importance.PixelImportance">PixelImportance</a></p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="omnixai.explainers.vision.GradCAM">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">omnixai.explainers.vision.</span></span><span class="sig-name descname"><span class="pre">GradCAM</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">model</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">target_layer</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">preprocess_function</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">mode</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'classification'</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#omnixai.explainers.vision.GradCAM" title="Permalink to this definition"></a></dt>
<dd><p>Bases: <a class="reference internal" href="omnixai.explainers.html#omnixai.explainers.base.ExplainerBase" title="omnixai.explainers.base.ExplainerBase"><code class="xref py py-class docutils literal notranslate"><span class="pre">omnixai.explainers.base.ExplainerBase</span></code></a></p>
<p>The Grad-CAM method for generating visual explanations.
If using this explainer, please cite <cite>Grad-CAM: Visual Explanations from Deep Networks
via Gradient-based Localization, Selvaraju et al., https://arxiv.org/abs/1610.02391</cite>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>model</strong> – The model whose type can be <cite>tf.keras.Model</cite> or <cite>torch.nn.Module</cite>.</p></li>
<li><p><strong>target_layer</strong> – The target layer for explanation, which can be
<cite>tf.keras.layers.Layer</cite> or <cite>torch.nn.Module</cite>.</p></li>
<li><p><strong>preprocess_function</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Callable</span></code>) – The preprocessing function that converts the raw data
into the inputs of <code class="docutils literal notranslate"><span class="pre">model</span></code>.</p></li>
<li><p><strong>mode</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>) – The task type, e.g., <cite>classification</cite> or <cite>regression</cite>.</p></li>
<li><p><strong>kwargs</strong> – Not used.</p></li>
</ul>
</dd>
</dl>
<dl class="py attribute">
<dt class="sig sig-object py" id="omnixai.explainers.vision.GradCAM.explanation_type">
<span class="sig-name descname"><span class="pre">explanation_type</span></span><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">'local'</span></em><a class="headerlink" href="#omnixai.explainers.vision.GradCAM.explanation_type" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="omnixai.explainers.vision.GradCAM.alias">
<span class="sig-name descname"><span class="pre">alias</span></span><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">['gradcam',</span> <span class="pre">'grad-cam']</span></em><a class="headerlink" href="#omnixai.explainers.vision.GradCAM.alias" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="omnixai.explainers.vision.GradCAM.explain">
<span class="sig-name descname"><span class="pre">explain</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">y</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#omnixai.explainers.vision.GradCAM.explain" title="Permalink to this definition"></a></dt>
<dd><p>Generates the explanations for the input instances.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>X</strong> (<a class="reference internal" href="omnixai.data.html#omnixai.data.image.Image" title="omnixai.data.image.Image"><code class="xref py py-class docutils literal notranslate"><span class="pre">Image</span></code></a>) – A batch of input instances.</p></li>
<li><p><strong>y</strong> – A batch of labels to explain. For regression, <code class="docutils literal notranslate"><span class="pre">y</span></code> is ignored.
For classification, the top predicted label of each input instance will be explained
when <cite>y = None</cite>.</p></li>
<li><p><strong>kwargs</strong> – Additional parameters.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>The explanations for all the instances, e.g., pixel importance scores.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><a class="reference internal" href="omnixai.explanations.image.html#omnixai.explanations.image.pixel_importance.PixelImportance" title="omnixai.explanations.image.pixel_importance.PixelImportance">PixelImportance</a></p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="omnixai.explainers.vision.GradCAMPlus">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">omnixai.explainers.vision.</span></span><span class="sig-name descname"><span class="pre">GradCAMPlus</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">model</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">target_layer</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">preprocess_function</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">mode</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'classification'</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#omnixai.explainers.vision.GradCAMPlus" title="Permalink to this definition"></a></dt>
<dd><p>Bases: <a class="reference internal" href="omnixai.explainers.html#omnixai.explainers.base.ExplainerBase" title="omnixai.explainers.base.ExplainerBase"><code class="xref py py-class docutils literal notranslate"><span class="pre">omnixai.explainers.base.ExplainerBase</span></code></a></p>
<p>The Grad-CAM++ method for generating visual explanations.
If using this explainer, please cite <cite>Grad-CAM++: Improved Visual Explanations for
Deep Convolutional Networks, Chattopadhyay et al., https://arxiv.org/pdf/1710.11063</cite>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>model</strong> – The model whose type can be <cite>tf.keras.Model</cite> or <cite>torch.nn.Module</cite>.</p></li>
<li><p><strong>target_layer</strong> – The target layer for explanation, which can be
<cite>tf.keras.layers.Layer</cite> or <cite>torch.nn.Module</cite>.</p></li>
<li><p><strong>preprocess_function</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Callable</span></code>) – The preprocessing function that converts the raw data
into the inputs of <code class="docutils literal notranslate"><span class="pre">model</span></code>.</p></li>
<li><p><strong>mode</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>) – The task type, e.g., <cite>classification</cite> or <cite>regression</cite>.</p></li>
<li><p><strong>kwargs</strong> – Not used.</p></li>
</ul>
</dd>
</dl>
<dl class="py attribute">
<dt class="sig sig-object py" id="omnixai.explainers.vision.GradCAMPlus.explanation_type">
<span class="sig-name descname"><span class="pre">explanation_type</span></span><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">'local'</span></em><a class="headerlink" href="#omnixai.explainers.vision.GradCAMPlus.explanation_type" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="omnixai.explainers.vision.GradCAMPlus.alias">
<span class="sig-name descname"><span class="pre">alias</span></span><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">['gradcam++',</span> <span class="pre">'grad-cam++']</span></em><a class="headerlink" href="#omnixai.explainers.vision.GradCAMPlus.alias" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="omnixai.explainers.vision.GradCAMPlus.explain">
<span class="sig-name descname"><span class="pre">explain</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">y</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#omnixai.explainers.vision.GradCAMPlus.explain" title="Permalink to this definition"></a></dt>
<dd><p>Generates the explanations for the input instances.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>X</strong> (<a class="reference internal" href="omnixai.data.html#omnixai.data.image.Image" title="omnixai.data.image.Image"><code class="xref py py-class docutils literal notranslate"><span class="pre">Image</span></code></a>) – A batch of input instances.</p></li>
<li><p><strong>y</strong> – A batch of labels to explain. For regression, <code class="docutils literal notranslate"><span class="pre">y</span></code> is ignored.
For classification, the top predicted label of each input instance will be explained
when <cite>y = None</cite>.</p></li>
<li><p><strong>kwargs</strong> – Additional parameters.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>The explanations for all the instances, e.g., pixel importance scores.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><a class="reference internal" href="omnixai.explanations.image.html#omnixai.explanations.image.pixel_importance.PixelImportance" title="omnixai.explanations.image.pixel_importance.PixelImportance">PixelImportance</a></p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="omnixai.explainers.vision.ContrastiveExplainer">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">omnixai.explainers.vision.</span></span><span class="sig-name descname"><span class="pre">ContrastiveExplainer</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">model</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">preprocess_function</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">mode</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'classification'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">background_data</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">c</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">10.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">beta</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">gamma</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">kappa</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">10.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">ae_model</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">binary_search_steps</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">5</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">learning_rate</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.01</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">num_iterations</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1000</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">grad_clip</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1000.0</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#omnixai.explainers.vision.ContrastiveExplainer" title="Permalink to this definition"></a></dt>
<dd><p>Bases: <a class="reference internal" href="omnixai.explainers.html#omnixai.explainers.base.ExplainerBase" title="omnixai.explainers.base.ExplainerBase"><code class="xref py py-class docutils literal notranslate"><span class="pre">omnixai.explainers.base.ExplainerBase</span></code></a></p>
<p>The contrastive explainer for image data.
If using this explainer, please cite the original work: <a class="reference external" href="https://arxiv.org/abs/1802.07623">https://arxiv.org/abs/1802.07623</a>.
This explainer only supports classification tasks.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>model</strong> – The classification model whose type is <cite>torch.nn.Module</cite> or <cite>tf.keras.Model</cite>.</p></li>
<li><p><strong>preprocess_function</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Callable</span></code>) – The pre-processing function that converts the raw input data
into the inputs of <code class="docutils literal notranslate"><span class="pre">model</span></code>.</p></li>
<li><p><strong>mode</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>) – It can be <cite>classification</cite> only.</p></li>
<li><p><strong>background_data</strong> (<a class="reference internal" href="omnixai.data.html#omnixai.data.image.Image" title="omnixai.data.image.Image"><code class="xref py py-class docutils literal notranslate"><span class="pre">Image</span></code></a>) – Sampled images for estimating background values.</p></li>
<li><p><strong>c</strong> – The weight of the loss term.</p></li>
<li><p><strong>beta</strong> – The weight of the L1 regularization term.</p></li>
<li><p><strong>gamma</strong> – The weight of the AE regularization term.</p></li>
<li><p><strong>kappa</strong> – The parameter in the hinge loss function.</p></li>
<li><p><strong>ae_model</strong> – The auto-encoder model used for regularization.</p></li>
<li><p><strong>binary_search_steps</strong> – The number of iterations to adjust the weight of the loss term.</p></li>
<li><p><strong>learning_rate</strong> – The learning rate.</p></li>
<li><p><strong>num_iterations</strong> – The maximum number of iterations during optimization.</p></li>
<li><p><strong>grad_clip</strong> – The value for clipping gradients.</p></li>
<li><p><strong>kwargs</strong> – Not used.</p></li>
</ul>
</dd>
</dl>
<dl class="py attribute">
<dt class="sig sig-object py" id="omnixai.explainers.vision.ContrastiveExplainer.explanation_type">
<span class="sig-name descname"><span class="pre">explanation_type</span></span><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">'local'</span></em><a class="headerlink" href="#omnixai.explainers.vision.ContrastiveExplainer.explanation_type" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="omnixai.explainers.vision.ContrastiveExplainer.alias">
<span class="sig-name descname"><span class="pre">alias</span></span><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">['cem',</span> <span class="pre">'contrastive']</span></em><a class="headerlink" href="#omnixai.explainers.vision.ContrastiveExplainer.alias" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="omnixai.explainers.vision.ContrastiveExplainer.explain">
<span class="sig-name descname"><span class="pre">explain</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#omnixai.explainers.vision.ContrastiveExplainer.explain" title="Permalink to this definition"></a></dt>
<dd><p>Generates the explanations corresponding to the input images.
Note that the returned results including the original input images,
the pertinent negatives and the pertinent positives have been processed
by the <code class="docutils literal notranslate"><span class="pre">preprocess_function</span></code>, e.g., if the <code class="docutils literal notranslate"><span class="pre">preprocess_function</span></code> rescales
[0, 255] to [0, 1], the return results will have range [0, 1].</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>X</strong> (<a class="reference internal" href="omnixai.data.html#omnixai.data.image.Image" title="omnixai.data.image.Image"><code class="xref py py-class docutils literal notranslate"><span class="pre">Image</span></code></a>) – A batch of the input images.</p></li>
<li><p><strong>kwargs</strong> – Not used here.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>The explanations for all the images, e.g., pertinent negatives and pertinent positives.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><a class="reference internal" href="omnixai.explanations.image.html#omnixai.explanations.image.contrast.ContrastiveExplanation" title="omnixai.explanations.image.contrast.ContrastiveExplanation">ContrastiveExplanation</a></p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="omnixai.explainers.vision.CounterfactualExplainer">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">omnixai.explainers.vision.</span></span><span class="sig-name descname"><span class="pre">CounterfactualExplainer</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">model</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">preprocess_function</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">mode</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'classification'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">c</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">10.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">kappa</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">10.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">binary_search_steps</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">5</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">learning_rate</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.01</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">num_iterations</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1000</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">grad_clip</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1000.0</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#omnixai.explainers.vision.CounterfactualExplainer" title="Permalink to this definition"></a></dt>
<dd><p>Bases: <a class="reference internal" href="omnixai.explainers.html#omnixai.explainers.base.ExplainerBase" title="omnixai.explainers.base.ExplainerBase"><code class="xref py py-class docutils literal notranslate"><span class="pre">omnixai.explainers.base.ExplainerBase</span></code></a></p>
<p>The counterfactual explainer for image data.
If using this explainer, please cite the paper <cite>Counterfactual Explanations without
Opening the Black Box: Automated Decisions and the GDPR, Sandra Wachter, Brent Mittelstadt, Chris Russell,
https://arxiv.org/abs/1711.00399</cite>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>model</strong> – The classification model which can be <cite>torch.nn.Module</cite> or <cite>tf.keras.Model</cite>.</p></li>
<li><p><strong>preprocess_function</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Callable</span></code>) – The preprocessing function that converts the raw data
into the inputs of <code class="docutils literal notranslate"><span class="pre">model</span></code>.</p></li>
<li><p><strong>mode</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>) – It can be <cite>classification</cite> only.</p></li>
<li><p><strong>c</strong> – The weight of the hinge loss term.</p></li>
<li><p><strong>kappa</strong> – The parameter in the hinge loss function.</p></li>
<li><p><strong>binary_search_steps</strong> – The number of iterations to adjust the weight of the loss term.</p></li>
<li><p><strong>learning_rate</strong> – The learning rate.</p></li>
<li><p><strong>num_iterations</strong> – The maximum number of iterations during optimization.</p></li>
<li><p><strong>grad_clip</strong> – The value for clipping gradients.</p></li>
<li><p><strong>kwargs</strong> – Not used.</p></li>
</ul>
</dd>
</dl>
<dl class="py attribute">
<dt class="sig sig-object py" id="omnixai.explainers.vision.CounterfactualExplainer.explanation_type">
<span class="sig-name descname"><span class="pre">explanation_type</span></span><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">'local'</span></em><a class="headerlink" href="#omnixai.explainers.vision.CounterfactualExplainer.explanation_type" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="omnixai.explainers.vision.CounterfactualExplainer.alias">
<span class="sig-name descname"><span class="pre">alias</span></span><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">['ce',</span> <span class="pre">'counterfactual']</span></em><a class="headerlink" href="#omnixai.explainers.vision.CounterfactualExplainer.alias" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="omnixai.explainers.vision.CounterfactualExplainer.explain">
<span class="sig-name descname"><span class="pre">explain</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#omnixai.explainers.vision.CounterfactualExplainer.explain" title="Permalink to this definition"></a></dt>
<dd><p>Generates the counterfactual explanations for the input images.
Note that the returned results including the original input images and the
counterfactual images have been processed by the <code class="docutils literal notranslate"><span class="pre">preprocess_function</span></code>,
e.g., if the <code class="docutils literal notranslate"><span class="pre">preprocess_function</span></code> rescales [0, 255] to [0, 1], the return
results will have range [0, 1].</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>X</strong> (<a class="reference internal" href="omnixai.data.html#omnixai.data.image.Image" title="omnixai.data.image.Image"><code class="xref py py-class docutils literal notranslate"><span class="pre">Image</span></code></a>) – A batch of the input images.</p></li>
<li><p><strong>kwargs</strong> – Not used here.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>The counterfactual explanations for all the images, e.g., counterfactual images.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><a class="reference internal" href="omnixai.explanations.image.html#omnixai.explanations.image.counterfactual.CounterfactualExplanation" title="omnixai.explanations.image.counterfactual.CounterfactualExplanation">CounterfactualExplanation</a></p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<section id="module-omnixai.explainers.vision.auto">
<span id="omnixai-explainers-vision-auto-module"></span><h2>omnixai.explainers.vision.auto module<a class="headerlink" href="#module-omnixai.explainers.vision.auto" title="Permalink to this headline"></a></h2>
<dl class="py class">
<dt class="sig sig-object py" id="omnixai.explainers.vision.auto.VisionExplainer">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">omnixai.explainers.vision.auto.</span></span><span class="sig-name descname"><span class="pre">VisionExplainer</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">explainers</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">mode</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">model</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">data</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">preprocess</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">postprocess</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">params</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#omnixai.explainers.vision.auto.VisionExplainer" title="Permalink to this definition"></a></dt>
<dd><p>Bases: <a class="reference internal" href="omnixai.explainers.html#omnixai.explainers.base.AutoExplainerBase" title="omnixai.explainers.base.AutoExplainerBase"><code class="xref py py-class docutils literal notranslate"><span class="pre">omnixai.explainers.base.AutoExplainerBase</span></code></a></p>
<p>The class derived from <cite>AutoExplainerBase</cite> for image data,
allowing users to choose multiple explainers and generate
different explanations at the same time.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">explainer</span> <span class="o">=</span> <span class="n">VisionExplainer</span><span class="p">(</span>
    <span class="n">explainers</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;gradcam&quot;</span><span class="p">,</span> <span class="s2">&quot;lime&quot;</span><span class="p">,</span> <span class="s2">&quot;ig&quot;</span><span class="p">],</span>
    <span class="n">mode</span><span class="o">=</span><span class="s2">&quot;classification&quot;</span><span class="p">,</span>
    <span class="n">model</span><span class="o">=</span><span class="n">model</span><span class="p">,</span>
    <span class="n">preprocess</span><span class="o">=</span><span class="n">preprocess_function</span><span class="p">,</span>
    <span class="n">postprocess</span><span class="o">=</span><span class="n">postprocess_function</span><span class="p">,</span>
    <span class="n">params</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;gradcam&quot;</span><span class="p">:</span> <span class="p">{</span><span class="s2">&quot;target_layer&quot;</span><span class="p">:</span> <span class="n">model</span><span class="o">.</span><span class="n">layer4</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]}}</span>
<span class="p">)</span>
<span class="n">local_explanations</span> <span class="o">=</span> <span class="n">explainer</span><span class="o">.</span><span class="n">explain</span><span class="p">(</span><span class="n">img</span><span class="p">)</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>explainers</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">Collection</span></code>) – The names or alias of the explainers to use.</p></li>
<li><p><strong>mode</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>) – The task type, e.g. classification or regression.</p></li>
<li><p><strong>model</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Any</span></code>) – The machine learning model which can be a scikit-learn model,
a tensorflow model, a torch model, or a prediction function.</p></li>
<li><p><strong>data</strong> (<a class="reference internal" href="omnixai.data.html#omnixai.data.image.Image" title="omnixai.data.image.Image"><code class="xref py py-class docutils literal notranslate"><span class="pre">Image</span></code></a>) – The training data used to initialize explainers.
It can be empty, e.g., <cite>data = Image()</cite>, for those explainers such as
<cite>IntegratedGradient</cite> and <cite>Grad-CAM</cite> that don’t require training data.</p></li>
<li><p><strong>preprocess</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Optional</span></code>[<code class="xref py py-data docutils literal notranslate"><span class="pre">Callable</span></code>]) – The preprocessing function that converts the raw data
into the inputs of <code class="docutils literal notranslate"><span class="pre">model</span></code>.</p></li>
<li><p><strong>postprocess</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Optional</span></code>[<code class="xref py py-data docutils literal notranslate"><span class="pre">Callable</span></code>]) – The postprocessing function that transforms the outputs of <code class="docutils literal notranslate"><span class="pre">model</span></code>
to a user-specific form, e.g., the predicted probability for each class.</p></li>
<li><p><strong>params</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Optional</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">Dict</span></code>]) – A dict containing the additional parameters for initializing each explainer,
e.g., <cite>params[“lime”] = {“param_1”: param_1, …}</cite>.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

</section>
<section id="subpackages">
<h2>Subpackages<a class="headerlink" href="#subpackages" title="Permalink to this headline"></a></h2>
<div class="toctree-wrapper compound">
<ul>
<li class="toctree-l1"><a class="reference internal" href="omnixai.explainers.vision.agnostic.html">omnixai.explainers.vision.agnostic package</a><ul>
<li class="toctree-l2"><a class="reference internal" href="omnixai.explainers.vision.agnostic.html#module-omnixai.explainers.vision.agnostic.lime">omnixai.explainers.vision.agnostic.lime module</a></li>
<li class="toctree-l2"><a class="reference internal" href="omnixai.explainers.vision.agnostic.html#module-omnixai.explainers.vision.agnostic.shap">omnixai.explainers.vision.agnostic.shap module</a></li>
<li class="toctree-l2"><a class="reference internal" href="omnixai.explainers.vision.agnostic.html#module-omnixai.explainers.vision.agnostic.pdp">omnixai.explainers.vision.agnostic.pdp module</a></li>
<li class="toctree-l2"><a class="reference internal" href="omnixai.explainers.vision.agnostic.html#module-omnixai.explainers.vision.agnostic.l2x">omnixai.explainers.vision.agnostic.l2x module</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="omnixai.explainers.vision.specific.html">omnixai.explainers.vision.specific package</a><ul>
<li class="toctree-l2"><a class="reference internal" href="omnixai.explainers.vision.specific.html#module-omnixai.explainers.vision.specific.ig">omnixai.explainers.vision.specific.ig module</a></li>
<li class="toctree-l2"><a class="reference internal" href="omnixai.explainers.vision.specific.html#module-omnixai.explainers.vision.specific.gradcam.gradcam">omnixai.explainers.vision.specific.gradcam.gradcam module</a></li>
<li class="toctree-l2"><a class="reference internal" href="omnixai.explainers.vision.specific.html#module-omnixai.explainers.vision.specific.cem">omnixai.explainers.vision.specific.cem module</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="omnixai.explainers.vision.counterfactual.html">omnixai.explainers.vision.counterfactual package</a><ul>
<li class="toctree-l2"><a class="reference internal" href="omnixai.explainers.vision.counterfactual.html#module-omnixai.explainers.vision.counterfactual.ce">omnixai.explainers.vision.counterfactual.ce module</a></li>
</ul>
</li>
</ul>
</div>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="omnixai.explainers.tabular.specific.html" class="btn btn-neutral float-left" title="omnixai.explainers.tabular.specific package" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="omnixai.explainers.vision.agnostic.html" class="btn btn-neutral float-right" title="omnixai.explainers.vision.agnostic package" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2022, salesforce.com, inc..</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  

  <div class="rst-versions" data-toggle="rst-versions" role="note" aria-label="versions">
    <span class="rst-current-version" data-toggle="rst-current-version">
      <span class="fa fa-book"> Versions</span>
      latest
      <span class="fa fa-caret-down"></span>
    </span>
    <div class="rst-other-versions">
      
      <dl>
        <dt>Versions</dt>
        
           <strong> 
          <dd><a href="">latest</a></dd>
           </strong> 
        
      </dl>
      
    </div>
  </div>

 <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>